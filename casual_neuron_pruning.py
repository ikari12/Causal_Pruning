# British English comments.
import torch
import torch.nn as nn
from transformers import AutoModel, AutoTokenizer
import pandas as pd
from pathlib import Path
from tqdm import tqdm
from mteb import MTEB
import time

# ============================================================================
# 1. Configuration
# ============================================================================
MODEL_NAME = "cl-nagoya/ruri-base-v2"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
RESULTS_DIR = "/app/results"

# Path to the analysis results file generated by the previous script.
SUMMARY_CSV_PATH = Path(RESULTS_DIR) / "comprehensive_neuron_summary_all.csv"

# ============================================================================
# 2. Helper Function to Calculate Sparsity
# ============================================================================
def report_sparsity(model: nn.Module, description: str):
    """Calculates and prints the sparsity of the model."""
    total_params = 0
    nonzero_params = 0
    for param in model.parameters():
        if param.requires_grad:
            total_params += param.numel()
            nonzero_params += torch.count_nonzero(param).item()
    
    sparsity = (1 - (nonzero_params / total_params)) * 100
    print(f"--- Sparsity Report for: {description} ---")
    print(f"  Total parameters:     {total_params:,}")
    print(f"  Non-zero parameters:  {nonzero_params:,}")
    print(f"  Model sparsity:       {sparsity:.2f}%")
    print("-" * 40)
    # â–¼â–¼â–¼ CHANGE 1: Return the calculated sparsity value â–¼â–¼â–¼
    return sparsity

# ============================================================================
# 3. Core Pruning Function
# ============================================================================
def prune_model_with_causal_scores(model: nn.Module, summary_df: pd.DataFrame, sparsity: float):
    """
    Prunes the model by zeroing out weights connected to causally unimportant neurons.
    """
    if not 0 <= sparsity < 1:
        print(f"âš ï¸ Sparsity must be between 0 and 1. Got {sparsity}. Skipping pruning.")
        return model
    
    if sparsity == 0:
        print("âœ… Sparsity is 0, skipping pruning.")
        return model

    print(f"âœ‚ï¸ Starting pruning for {sparsity*100:.1f}% sparsity...")

    score_threshold = summary_df['causal_score'].quantile(q=sparsity)
    neurons_to_prune = summary_df[summary_df['causal_score'] < score_threshold]
    print(f"Identified {len(neurons_to_prune)} neurons to prune (score < {score_threshold:.6f}).")

    n_heads = model.config.num_attention_heads
    d_head = model.config.hidden_size // n_heads

    with torch.no_grad():
        for _, neuron in tqdm(neurons_to_prune.iterrows(), total=len(neurons_to_prune), desc="Pruning neurons"):
            layer, n_type = neuron['layer'], neuron['type']
            if n_type == 'ATTN':
                head, dim = neuron['head'], neuron['dim']
                w_o = model.encoder.layer[layer].attention.output.dense.weight
                start_col = head * d_head + dim
                if start_col < w_o.shape[1]:
                    w_o.data[:, start_col] = 0.
            elif n_type == 'FFN':
                dim = neuron['dim']
                w_in = model.encoder.layer[layer].intermediate.dense.weight
                if dim < w_in.shape[0]:
                    w_in.data[dim, :] = 0.
                w_out = model.encoder.layer[layer].output.dense.weight
                if dim < w_out.shape[1]:
                    w_out.data[:, dim] = 0.

    print("âœ… Pruning complete.")
    return model

# ============================================================================
# 4. Performance Evaluation Function
# ============================================================================
def evaluate_model(model_to_eval, tokenizer_to_use, sparsity_level):
    """
    Receives a model and tokenizer, then evaluates its performance on the JSTS benchmark.
    """
    print(f"\n--- ðŸ”¬ Starting Performance Evaluation on JSTS for {sparsity_level*100:.0f}% sparsity ---")

    class ModelWrapper:
        def __init__(self, model, tokenizer):
            self.model = model
            self.tokenizer = tokenizer
        def encode(self, sentences, batch_size=32, **kwargs):
            all_embeddings = []
            for i in range(0, len(sentences), batch_size):
                batch = sentences[i:i+batch_size]
                inputs = self.tokenizer(batch, padding=True, truncation=True, return_tensors='pt', max_length=512)
                inputs = {k: v.to(self.model.device) for k, v in inputs.items()}
                with torch.no_grad():
                    outputs = self.model(**inputs)
                    last_hidden = outputs.last_hidden_state
                    mask = inputs['attention_mask'].unsqueeze(-1)
                    sum_hidden = torch.sum(last_hidden * mask, dim=1)
                    sum_mask = torch.sum(mask, dim=1)
                    embeddings = sum_hidden / sum_mask
                all_embeddings.append(embeddings.cpu())
            return torch.cat(all_embeddings)

    model_for_eval = ModelWrapper(model_to_eval, tokenizer_to_use)
    evaluation = MTEB(tasks=["JSTS"], task_langs=["ja"])
    results = evaluation.run(
        model_for_eval, 
        output_folder=f"{RESULTS_DIR}/JSTS_pruned_{sparsity_level*100:.0f}percent", 
        eval_splits=["validation"],
    )
    print(f"--- âœ… Evaluation Complete for {sparsity_level*100:.0f}% ---")
    pearson_score = results[0].scores['validation'][0]['pearson']
    return pearson_score

# ============================================================================
# 5. Main Execution Block
# ============================================================================
if __name__ == "__main__":
    
    if not SUMMARY_CSV_PATH.exists():
        raise FileNotFoundError(f"Analysis results not found at '{SUMMARY_CSV_PATH}'. Run analysis script first.")
    
    print("Loading and pre-processing analysis data...")
    neuron_summary_df = pd.read_csv(SUMMARY_CSV_PATH)
    neuron_summary_df['dim'] = neuron_summary_df['neuron_id'].str.extract(r'D(\d+)').astype(int)
    neuron_summary_df['head'] = neuron_summary_df['neuron_id'].str.extract(r'H(\d+)').astype('Int64')
    print("âœ… Analysis data loaded.")

    sparsity_levels = [i / 10.0 for i in range(10)] # 0.0, 0.1, ..., 0.9
    performance_results = []

    for sparsity in sparsity_levels:
        print("\n" + "="*60)
        print(f"PROCESSING SPARSITY LEVEL: {sparsity*100:.0f}%")
        print("="*60)

        print("Loading original model...")
        model = AutoModel.from_pretrained(MODEL_NAME).to(DEVICE)
        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
        
        pruned_model = prune_model_with_causal_scores(model, neuron_summary_df.copy(), sparsity)
        
        # â–¼â–¼â–¼ CHANGE 2: Capture the returned value and add it to the results list â–¼â–¼â–¼
        actual_sparsity = report_sparsity(pruned_model, f"Pruned Model ({sparsity*100:.0f}%)")

        start_time = time.time()
        score = evaluate_model(pruned_model, tokenizer, sparsity)
        end_time = time.time()
        
        performance_results.append({
            "target_sparsity": f"{sparsity*100:.0f}%",
            "actual_sparsity": f"{actual_sparsity:.2f}%", # Added this line
            "jsts_pearson_score": score,
            "evaluation_time_sec": end_time - start_time
        })
        # â–²â–²â–² CHANGE 2 ENDS HERE â–²â–²â–²

    print("\n\n" + "="*60)
    print("ðŸ“Š FINAL PERFORMANCE REPORT")
    print("="*60)
    
    results_df = pd.DataFrame(performance_results)
    print(results_df.to_string(index=False))